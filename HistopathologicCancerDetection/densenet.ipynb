{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import  tensorflow as tf\n",
    "from    tensorflow import keras\n",
    "from    tensorflow.keras import layers, Sequential\n",
    "from    tensorflow.keras.callbacks import EarlyStopping \n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of train labels: 174865, 71308/103557\n",
      "Amount of cv labels: 45160, 17809/27351\n",
      "Percentage of cv labels: 0.20524940347687762\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def return_tumor_or_not(dic, one_id):\n",
    "    return dic[one_id]\n",
    "\n",
    "def create_dict():\n",
    "    df = pd.read_csv(\"data/train_labels.csv\")\n",
    "    result_dict = {}\n",
    "    for index in range(df.shape[0]):\n",
    "        one_id = df.iloc[index,0]\n",
    "        tumor_or_not = df.iloc[index,1]\n",
    "        result_dict[one_id] = int(tumor_or_not)\n",
    "    return result_dict\n",
    "\n",
    "def find_missing(train_ids, cv_ids):\n",
    "    all_ids = set(pd.read_csv(\"data/train_labels.csv\")['id'].values)\n",
    "    wsi_ids = set(train_ids + cv_ids)\n",
    "\n",
    "    missing_ids = list(all_ids-wsi_ids)\n",
    "    return missing_ids\n",
    "\n",
    "\n",
    "def generate_split():\n",
    "    ids = pd.read_csv(\"data/patch_id_wsi.csv\")\n",
    "    wsi_dict = {}\n",
    "    for i in range(ids.shape[0]):\n",
    "        wsi = ids.iloc[i,1]\n",
    "        train_id = ids.iloc[i,0]\n",
    "        wsi_array = wsi.split('_')\n",
    "        number = int(wsi_array[3])\n",
    "        if wsi_dict.get(number) is None:\n",
    "            wsi_dict[number] = [train_id]\n",
    "        else:\n",
    "            wsi_dict[number].append(train_id)\n",
    "\n",
    "    wsi_keys = list(wsi_dict.keys())\n",
    "    np.random.seed()\n",
    "    np.random.shuffle(wsi_keys)\n",
    "    amount_of_keys = len(wsi_keys)\n",
    "\n",
    "    keys_for_train = wsi_keys[0:int(amount_of_keys*0.8)]\n",
    "    keys_for_cv = wsi_keys[int(amount_of_keys*0.8):]\n",
    "    train_ids = []\n",
    "    cv_ids = []\n",
    "\n",
    "    for key in keys_for_train:\n",
    "        train_ids += wsi_dict[key]\n",
    "\n",
    "    for key in keys_for_cv:\n",
    "        cv_ids += wsi_dict[key]\n",
    "\n",
    "    dic = create_dict()\n",
    "\n",
    "    missing_ids = find_missing(train_ids, cv_ids)\n",
    "    missing_ids_total = len(missing_ids)\n",
    "    np.random.seed()\n",
    "    np.random.shuffle(missing_ids)\n",
    "\n",
    "    train_missing_ids = missing_ids[0:int(missing_ids_total*0.8)]\n",
    "    cv_missing_ids = missing_ids[int(missing_ids_total*0.8):]\n",
    "\n",
    "    train_ids += train_missing_ids\n",
    "    cv_ids += cv_missing_ids\n",
    "\n",
    "    train_labels = []\n",
    "    cv_labels = []\n",
    "\n",
    "    train_tumor = 0\n",
    "    for one_id in train_ids:\n",
    "        temp = return_tumor_or_not(dic, one_id)\n",
    "        train_tumor += temp\n",
    "        train_labels.append(temp)\n",
    "\n",
    "    cv_tumor = 0\n",
    "    for one_id in cv_ids:\n",
    "        temp = return_tumor_or_not(dic, one_id)\n",
    "        cv_tumor += temp\n",
    "        cv_labels.append(temp)\n",
    "    total = len(train_ids) + len(cv_ids)\n",
    "\n",
    "    print(\"Amount of train labels: {}, {}/{}\".format(len(train_ids), train_tumor, len(train_ids)-train_tumor))\n",
    "    print(\"Amount of cv labels: {}, {}/{}\".format(len(cv_ids), cv_tumor, len(cv_ids) - cv_tumor))\n",
    "    print(\"Percentage of cv labels: {}\".format(len(cv_ids)/total))\n",
    "\n",
    "    return train_ids, cv_ids, train_labels, cv_labels\n",
    "\n",
    "train_ids, cv_ids, train_labels, cv_labels = generate_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/train_labels.csv').values\n",
    "# df_train = df_train.sample(n=20, random_state=2019).values\n",
    "# kf = KFold(n_splits=2000, shuffle=True, random_state=2019)\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "def get_data_generator(train_df, valid_df):\n",
    "    datagen_train=ImageDataGenerator(\n",
    "        rescale=1./255.,\n",
    "        zoom_range=0.1,\n",
    "        rotation_range=90,\n",
    "#         zca_whitening=True,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True\n",
    "    )\n",
    "    datagen_valid=ImageDataGenerator(\n",
    "#         zca_whitening=True,\n",
    "        rescale=1./255.\n",
    "    )\n",
    "    train_generator = datagen_train.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        directory=r'e:/cancerDetection/train',\n",
    "        x_col='id',\n",
    "        y_col='label',\n",
    "        target_size=(96,96), \n",
    "        class_mode='binary', \n",
    "        batch_size=batch_size)\n",
    "\n",
    "    valid_generator = datagen_valid.flow_from_dataframe(\n",
    "        dataframe=valid_df,\n",
    "        directory=r'e:/cancerDetection/train',\n",
    "        x_col='id',\n",
    "        y_col='label',\n",
    "        target_size=(96,96), \n",
    "        class_mode='binary', \n",
    "        batch_size=batch_size)  #每次生成的样本数，注意：generator是无限循环的，需要在fit_generator中指定steps_per_epoch,才能知道一个epoch 什么时候结束\n",
    "    return train_generator, valid_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 96, 96, 3)]       0         \n",
      "_________________________________________________________________\n",
      "densenet121 (Model)          (None, 3, 3, 1024)        7037504   \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               4719104   \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 11,759,169\n",
      "Trainable params: 11,674,497\n",
      "Non-trainable params: 84,672\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "IN_SHAPE=(96,96,3)\n",
    "class BatchNormalization(tf.keras.layers.BatchNormalization):\n",
    "    \"\"\"\n",
    "    Make trainable=False freeze BN for real (the og version is sad)\n",
    "    \"\"\"\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        if training is None:\n",
    "            training = tf.constant(False)\n",
    "        training = tf.logical_and(training, self.trainable)\n",
    "        return super().call(x, training)\n",
    "    \n",
    "def model_bulid():\n",
    "    inputs = keras.Input(IN_SHAPE)\n",
    "    conv_base = DenseNet121(\n",
    "        weights='imagenet',\n",
    "#         weights=None,\n",
    "        include_top=False,\n",
    "        input_shape=IN_SHAPE\n",
    "    )\n",
    "    x = conv_base(inputs)\n",
    "    out = layers.Flatten()(x)\n",
    "    out = layers.Dropout(0.2)(out)\n",
    "    out = layers.Dense(512)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation(activation='relu')(out)\n",
    "    out = layers.Dense(1, activation=\"sigmoid\")(out)\n",
    "    model = keras.Model(inputs, out)\n",
    "\n",
    "#     conv_base.trainable = False\n",
    "#     set_trainable = False\n",
    "#     for layer in conv_base.layers[:]:\n",
    "#         layer.trainable = False\n",
    "#     for layer in conv_base.layers[-40:]:\n",
    "#         layer.trainable = True\n",
    "\n",
    "\n",
    "    model.compile(keras.optimizers.RMSprop(),\n",
    "                  loss=tf.losses.BinaryCrossentropy(from_logits=False), metrics=[\"accuracy\"])\n",
    "\n",
    "    model.summary()\n",
    "#     plot_model(model, to_file='NetStruct.png', show_shapes=True)\n",
    "    return model\n",
    "\n",
    "model = model_bulid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 96, 96, 3)]       0         \n",
      "_________________________________________________________________\n",
      "densenet121 (Model)          (None, 3, 3, 1024)        7037504   \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               4719104   \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 11,759,169\n",
      "Trainable params: 11,674,497\n",
      "Non-trainable params: 84,672\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "#     print(layer.name)\n",
    "    if layer.name=='densenet121':\n",
    "        layer.trainable=True\n",
    "#         print(len(layer.layers))\n",
    "        for l in layer.layers[0:80]:\n",
    "#             print(l.trainable)\n",
    "            l.trainable=False\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, valid = train_test_split(df_train, test_size=0.05, shuffle=True)\n",
    "train=np.array(list(zip(train_ids, train_labels)))\n",
    "valid = np.array(list(zip(cv_ids, cv_labels)))\n",
    "train_df = pd.DataFrame(train, columns=['id','label']).astype('str')\n",
    "valid_df = pd.DataFrame(valid, columns=['id','label']).astype('str')\n",
    "train_df.values[:,0]=train_df.values[:,0]+'.tif'\n",
    "valid_df.values[:,0]=valid_df.values[:,0]+'.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(keras.optimizers.Adam(learning_rate=0.0001), loss=tf.losses.BinaryCrossentropy(from_logits=False), metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard = TensorBoard(log_dir='./logs',  # log 目录\n",
    "#                       # histogram_freq=1,  # 按照何等频率（epoch）来计算直方图，0为不计算\n",
    "#                       # batch_size=batch_size,     # 用多大量的数据计算直方图\n",
    "#                       write_graph=True,  # 是否存储网络结构图\n",
    "#                       write_grads=False,  # 是否可视化梯度直方图\n",
    "#                       write_images=False,  # 是否可视化参数\n",
    "#                       embeddings_freq=0,\n",
    "#                       embeddings_layer_names=None,\n",
    "#                       embeddings_metadata=None)\n",
    "# model_checkpoint = ModelCheckpoint(\n",
    "#     'weights_epoch'+'_'+str(num_fold)+'.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# reducel = ReduceLROnPlateau(\n",
    "#     monitor='val_loss', patience=5, verbose=1, factor=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 174865 images belonging to 2 classes.\n",
      "Found 45160 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator, valid_generator = get_data_generator(train_df, valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2732/2732 [==============================] - 3006s 1s/step - loss: 0.1566 - accuracy: 0.9416 - val_loss: 0.6644 - val_accuracy: 0.8364\n",
      "Epoch 2/50\n",
      "2732/2732 [==============================] - 2989s 1s/step - loss: 0.1267 - accuracy: 0.9542 - val_loss: 0.3962 - val_accuracy: 0.8962\n",
      "Epoch 3/50\n",
      "2732/2732 [==============================] - 2985s 1s/step - loss: 0.1114 - accuracy: 0.9598 - val_loss: 0.4939 - val_accuracy: 0.8650\n",
      "Epoch 4/50\n",
      "2732/2732 [==============================] - 2925s 1s/step - loss: 0.0981 - accuracy: 0.9647 - val_loss: 0.8292 - val_accuracy: 0.8148\n",
      "Epoch 5/50\n",
      "2732/2732 [==============================] - 2942s 1s/step - loss: 0.0890 - accuracy: 0.9680 - val_loss: 0.5569 - val_accuracy: 0.8757\n",
      "Epoch 6/50\n",
      "2732/2732 [==============================] - 2974s 1s/step - loss: 0.0836 - accuracy: 0.9707 - val_loss: 0.8617 - val_accuracy: 0.8287\n",
      "Epoch 7/50\n",
      "2732/2732 [==============================] - 2940s 1s/step - loss: 0.0763 - accuracy: 0.9734 - val_loss: 0.8356 - val_accuracy: 0.8368\n",
      "Epoch 8/50\n",
      "2732/2732 [==============================] - 2939s 1s/step - loss: 0.0722 - accuracy: 0.9743 - val_loss: 0.5826 - val_accuracy: 0.8691\n",
      "Epoch 9/50\n",
      "2732/2732 [==============================] - 2943s 1s/step - loss: 0.0677 - accuracy: 0.9763 - val_loss: 1.9130 - val_accuracy: 0.7978\n",
      "Epoch 10/50\n",
      "2732/2732 [==============================] - 2951s 1s/step - loss: 0.0637 - accuracy: 0.9776 - val_loss: 1.0737 - val_accuracy: 0.8306\n",
      "Epoch 11/50\n",
      "2732/2732 [==============================] - 2931s 1s/step - loss: 0.0598 - accuracy: 0.9791 - val_loss: 0.3831 - val_accuracy: 0.9028\n",
      "Epoch 12/50\n",
      "2732/2732 [==============================] - 2954s 1s/step - loss: 0.0568 - accuracy: 0.9804 - val_loss: 0.6052 - val_accuracy: 0.8708\n",
      "Epoch 13/50\n",
      "2732/2732 [==============================] - 3015s 1s/step - loss: 0.0540 - accuracy: 0.9813 - val_loss: 1.4667 - val_accuracy: 0.8317\n",
      "Epoch 14/50\n",
      "2732/2732 [==============================] - 3028s 1s/step - loss: 0.0524 - accuracy: 0.9817 - val_loss: 1.1106 - val_accuracy: 0.8311\n",
      "Epoch 15/50\n",
      "2732/2732 [==============================] - 2953s 1s/step - loss: 0.0500 - accuracy: 0.9826 - val_loss: 0.7187 - val_accuracy: 0.8832\n",
      "Epoch 16/50\n",
      "2732/2732 [==============================] - 2955s 1s/step - loss: 0.0482 - accuracy: 0.9831 - val_loss: 1.2293 - val_accuracy: 0.8466\n",
      "Epoch 17/50\n",
      "2732/2732 [==============================] - 2960s 1s/step - loss: 0.0461 - accuracy: 0.9840 - val_loss: 3.2238 - val_accuracy: 0.7545\n",
      "Epoch 18/50\n",
      "2732/2732 [==============================] - 2988s 1s/step - loss: 0.0442 - accuracy: 0.9845 - val_loss: 1.3763 - val_accuracy: 0.8430\n",
      "Epoch 19/50\n",
      "2732/2732 [==============================] - 2987s 1s/step - loss: 0.0426 - accuracy: 0.9850 - val_loss: 0.9727 - val_accuracy: 0.8499\n",
      "Epoch 20/50\n",
      "2732/2732 [==============================] - 3008s 1s/step - loss: 0.0401 - accuracy: 0.9861 - val_loss: 1.2675 - val_accuracy: 0.8745\n",
      "Epoch 21/50\n",
      "2732/2732 [==============================] - 2980s 1s/step - loss: 0.0379 - accuracy: 0.9865 - val_loss: 1.0271 - val_accuracy: 0.8717\n",
      "Epoch 22/50\n",
      "2732/2732 [==============================] - 2938s 1s/step - loss: 0.0369 - accuracy: 0.9870 - val_loss: 0.9352 - val_accuracy: 0.8661\n",
      "Epoch 23/50\n",
      "2732/2732 [==============================] - 2979s 1s/step - loss: 0.0360 - accuracy: 0.9872 - val_loss: 0.9190 - val_accuracy: 0.8623\n",
      "Epoch 24/50\n",
      "2732/2732 [==============================] - 3056s 1s/step - loss: 0.0347 - accuracy: 0.9877 - val_loss: 1.2908 - val_accuracy: 0.8541\n",
      "Epoch 25/50\n",
      "2732/2732 [==============================] - 2972s 1s/step - loss: 0.0329 - accuracy: 0.9884 - val_loss: 0.9145 - val_accuracy: 0.8768\n",
      "Epoch 26/50\n",
      " 717/2732 [======>.......................] - ETA: 33:40 - loss: 0.0299 - accuracy: 0.9894"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-4dca3aea02ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearlystopper\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m       \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m       \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m      )\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\devsw\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1513\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1514\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1515\u001b[1;33m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m   1516\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1517\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[1;32md:\\devsw\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m       \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\devsw\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1256\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1257\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1258\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_fit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1259\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\devsw\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_make_fit_function\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2172\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_make_fit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2173\u001b[0m     metrics_tensors = [\n\u001b[1;32m-> 2174\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_all_stateful_metrics_tensors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2175\u001b[0m     ]\n\u001b[0;32m   2176\u001b[0m     self._make_train_function_helper(\n",
      "\u001b[1;32md:\\devsw\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2172\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_make_fit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2173\u001b[0m     metrics_tensors = [\n\u001b[1;32m-> 2174\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_all_stateful_metrics_tensors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2175\u001b[0m     ]\n\u001b[0;32m   2176\u001b[0m     self._make_train_function_helper(\n",
      "\u001b[1;32md:\\devsw\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_all_stateful_metrics_tensors\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1894\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1895\u001b[0m       \u001b[0mmetrics_tensors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compile_stateful_metrics_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1896\u001b[1;33m     \u001b[0mmetrics_tensors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_all_metrics_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1897\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmetrics_tensors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\devsw\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36m_all_metrics_tensors\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    716\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    717\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 718\u001b[1;33m         \u001b[0mmetrics_tensors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_all_metrics_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    719\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    720\u001b[0m         \u001b[0mmetrics_tensors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_metrics_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\devsw\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_all_metrics_tensors\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1885\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1886\u001b[0m       \u001b[0mmetrics_tensors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compile_metrics_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1887\u001b[1;33m     \u001b[0mmetrics_tensors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_all_metrics_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1888\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmetrics_tensors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1889\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\devsw\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36m_all_metrics_tensors\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    714\u001b[0m     \u001b[1;31m# TODO(psv): Remove this property.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m     \u001b[0mmetrics_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m         \u001b[0mmetrics_tensors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_all_metrics_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\devsw\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mlayers\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    487\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m     return trackable_layer_utils.filter_empty_layer_containers(\n\u001b[1;32m--> 489\u001b[1;33m         self._layers)\n\u001b[0m\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mget_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\devsw\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\layer_utils.py\u001b[0m in \u001b[0;36mfilter_empty_layer_containers\u001b[1;34m(layer_list)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexisting\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m       \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m     \u001b[0mexisting\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mis_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m       \u001b[0mfiltered\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\devsw\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\object_identity.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrap_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\devsw\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\object_identity.py\u001b[0m in \u001b[0;36m__hash__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;31m# id(weakref.ref(a)) == id(weakref.ref(a)) and weakref.ref(a) is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;31m# weakref.ref(a) in _WeakObjectIdentityWrapper.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "earlystopper = EarlyStopping(\n",
    "    monitor='val_loss', patience=50)\n",
    "\n",
    "history = model.fit_generator(train_generator,\n",
    "      validation_data=valid_generator,\n",
    "      epochs=50,\n",
    "      callbacks=[earlystopper],\n",
    "      steps_per_epoch=train_df.shape[0]//(batch_size),\n",
    "      validation_steps=valid_df.shape[0]//(batch_size)\n",
    "     )\n",
    "\n",
    "# plt.plot(history.history['loss'], label='train')\n",
    "# plt.plot(history.history['val_loss'], label='valid')\n",
    "# plt.title(\"model loss\")\n",
    "# plt.ylabel(\"loss\")\n",
    "# plt.xlabel(\"epoch\")\n",
    "# plt.legend([\"train\", \"valid\"], loc=\"upper left\")\n",
    "# plt.savefig('loss_performance'+'.png')\n",
    "# plt.clf()\n",
    "# plt.plot(history.history['acc'], label='train')\n",
    "# plt.plot(history.history['val_acc'], label='valid')\n",
    "# plt.title(\"model acc\")\n",
    "# plt.ylabel(\"acc\")\n",
    "# plt.xlabel(\"epoch\")\n",
    "# plt.legend([\"train\", \"valid\"], loc=\"upper left\")\n",
    "# plt.savefig('acc_performance'+'.png')\n",
    "\n",
    "# with open('logs_kfold.txt', 'a+') as f:\n",
    "#     f.write(str(datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\"))+'\\n')\n",
    "# #     f.write(str(num_fold)+'\\n')\n",
    "#     f.write(str(history.history['loss'])+'\\n')\n",
    "#     f.write(str(history.history['val_loss'])+'\\n')\n",
    "#     f.write(str(history.history['acc'])+'\\n')\n",
    "#     f.write(str(history.history['val_acc'])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "from skimage.io import imread\n",
    "testing_files = glob(os.path.join('data/test/','*.tif'))\n",
    "submission = pd.DataFrame()\n",
    "for index in range(0, len(testing_files), 128):\n",
    "    data_frame = pd.DataFrame({'path': testing_files[index:index+128]})\n",
    "    data_frame['id'] = data_frame.path.map(lambda x: x.split('/')[1].split(\".\")[0])\n",
    "    data_frame['image'] = data_frame['path'].map(imread)\n",
    "    images = np.stack(data_frame.image, axis=0)\n",
    "    predicted_labels = [model.predict(np.expand_dims(image, axis=0),verbose=0)[0][0] for image in images]\n",
    "    predictions = np.array(predicted_labels)\n",
    "    data_frame['label'] = predictions\n",
    "    submission = pd.concat([submission, data_frame[[\"id\", \"label\"]]])\n",
    "submission.to_csv('result.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['label']=submission['label'].map(lambda x: int(np.round(x)))\n",
    "submission.id=submission.id.map(lambda x: x.split('\\\\')[1])\n",
    "submission.to_csv('result.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00006537328c33e284c973d7b39d340809f7271b</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000ec92553fda4ce39889f9226ace43cae3364e</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00024a6dee61f12f7856b0fc6be20bc7a48ba3d2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000253dfaa0be9d0d100283b22284ab2f6b643f6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000270442cc15af719583a8172c87cd2bd9c7746</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id  label\n",
       "0  00006537328c33e284c973d7b39d340809f7271b      1\n",
       "1  0000ec92553fda4ce39889f9226ace43cae3364e      1\n",
       "2  00024a6dee61f12f7856b0fc6be20bc7a48ba3d2      0\n",
       "3  000253dfaa0be9d0d100283b22284ab2f6b643f6      0\n",
       "4  000270442cc15af719583a8172c87cd2bd9c7746      0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 57458 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen=ImageDataGenerator(\n",
    "#     zca_whitening=True,\n",
    "    rescale=1./255.\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory=r'E:/cancerDetection/test',\n",
    "    target_size=(96,96), \n",
    "    batch_size=2048,\n",
    "    class_mode=None,\n",
    "    shuffle=False\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0\n",
      "2048\n",
      "1\\00006537328c33e284c973d7b39d340809f7271b.tif 1\\0912a4f265fe7f37be287c4333167e59162cccba.tif\n",
      "2048\n",
      "2 2048\n",
      "2048\n",
      "1\\0912be906eeb7f21a21c9378c52640411c63fe92.tif 1\\126bf75fb455b0ead4c791b8616f91a7d7e0a23d.tif\n",
      "2048\n"
     ]
    }
   ],
   "source": [
    "submission = pd.DataFrame()\n",
    "\n",
    "for i in test_generator:\n",
    "\n",
    "    data_frame = pd.DataFrame()\n",
    "    \n",
    "    idx = (test_generator.batch_index - 1) * test_generator.batch_size\n",
    "    if test_generator.batch_index == 0:\n",
    "        idx = 57344\n",
    "    print(test_generator.batch_index, idx)\n",
    "    files = test_generator.filenames[idx : (idx + test_generator.batch_size) if (idx + test_generator.batch_size<=57458) else 57458] #\n",
    "    print(len(files))\n",
    "    print(files[0], files[-1])\n",
    "    data_frame['id']=np.array([x.split('.')[0][2:] for x in files])\n",
    "    predictions = model.predict(i,verbose=0)\n",
    "    print(len(predictions))\n",
    "    data_frame['label'] = np.array([int(x) for x in list(map(np.rint, predictions))])\n",
    "    submission = pd.concat([submission, data_frame])\n",
    "    if test_generator.batch_index == 0:\n",
    "        break\n",
    "\n",
    "submission.to_csv('result.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
